{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonkarel/selfmodifai/blob/main/notebooks/selfmodifai_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tloen/alpaca-lora.git"
      ],
      "metadata": {
        "id": "sBHQLj9Gyya3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"alpaca-lora\")"
      ],
      "metadata": {
        "id": "x2HUy2mQz_GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAgfE8uJ5mf0"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM"
      ],
      "metadata": {
        "id": "xvWFNzrJDqhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import fire\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "from utils.callbacks import Iteratorize, Stream\n",
        "from utils.prompter import Prompter\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:  # noqa: E722\n",
        "    pass\n",
        "\n",
        "\n",
        "def alpaca_generate(\n",
        "    instruction: str,\n",
        "    load_8bit: bool = False,\n",
        "    base_model: str = \"\",\n",
        "    lora_weights: str = \"tloen/alpaca-lora-7b\",\n",
        "    prompt_template: str = \"\",  # The prompt template to use, will default to alpaca.\n",
        "\n",
        "):\n",
        "    \"\"\"Modified from https://github.com/tloen/alpaca-lora/blob/main/generate.py\"\"\"\n",
        "    \n",
        "    base_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\n",
        "    assert (\n",
        "        base_model\n",
        "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
        "\n",
        "    prompter = Prompter(prompt_template)\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    if device == \"cuda\":\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            base_model,\n",
        "            load_in_8bit=load_8bit,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "    elif device == \"mps\":\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            base_model,\n",
        "            device_map={\"\": device},\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            device_map={\"\": device},\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "    else:\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            device_map={\"\": device},\n",
        "        )\n",
        "\n",
        "    # unwind broken decapoda-research config\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "    model.config.bos_token_id = 1\n",
        "    model.config.eos_token_id = 2\n",
        "\n",
        "    if not load_8bit:\n",
        "        model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "    model.eval()\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    def evaluate(\n",
        "        instruction,\n",
        "        input=None,\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        top_k=40,\n",
        "        num_beams=4,\n",
        "        max_new_tokens=128,\n",
        "        stream_output=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        prompt = prompter.generate_prompt(instruction, input)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            num_beams=num_beams,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        generate_params = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"generation_config\": generation_config,\n",
        "            \"return_dict_in_generate\": True,\n",
        "            \"output_scores\": True,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "        }\n",
        "\n",
        "        if stream_output:\n",
        "            # Stream the reply 1 token at a time.\n",
        "            # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
        "            # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
        "\n",
        "            def generate_with_callback(callback=None, **kwargs):\n",
        "                kwargs.setdefault(\n",
        "                    \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
        "                )\n",
        "                kwargs[\"stopping_criteria\"].append(\n",
        "                    Stream(callback_func=callback)\n",
        "                )\n",
        "                with torch.no_grad():\n",
        "                    model.generate(**kwargs)\n",
        "\n",
        "            def generate_with_streaming(**kwargs):\n",
        "                return Iteratorize(\n",
        "                    generate_with_callback, kwargs, callback=None\n",
        "                )\n",
        "\n",
        "            with generate_with_streaming(**generate_params) as generator:\n",
        "                for output in generator:\n",
        "                    # new_tokens = len(output) - len(input_ids[0])\n",
        "                    decoded_output = tokenizer.decode(output)\n",
        "\n",
        "                    if output[-1] in [tokenizer.eos_token_id]:\n",
        "                        break\n",
        "\n",
        "                    yield prompter.get_response(decoded_output)\n",
        "            return  # early return for stream_output\n",
        "\n",
        "        # Without streaming\n",
        "        with torch.no_grad():\n",
        "            generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "            )\n",
        "        s = generation_output.sequences[0]\n",
        "        output = tokenizer.decode(s)\n",
        "        yield prompter.get_response(output)\n",
        "\n",
        "    return evaluate(instruction)"
      ],
      "metadata": {
        "id": "ZMD607YduNq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLLM(LLM):\n",
        "            \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "    \n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "        \n",
        "        \n",
        "        alpaca_generator = alpaca_generate(instruction=prompt, load_8bit=True, base_model='decapoda-research/llama-7b-hf')\n",
        "        output = next(alpaca_generator)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XjlIN1lzDre4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CustomLLM()"
      ],
      "metadata": {
        "id": "qIO_1I9baV_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI\n",
        "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
        "from langchain.utilities import PythonREPL\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import load_tools"
      ],
      "metadata": {
        "id": "rQjMtHoo5vcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"python_repl\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "input = \"You are the large language model Alpaca-LoRA. Your code is in the current directory. Modify your own code so that you are a more powerful language model.\""
      ],
      "metadata": {
        "id": "oK08UxA1oX0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(input)"
      ],
      "metadata": {
        "id": "F8qLg-e-Bvip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}